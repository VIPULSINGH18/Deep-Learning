# -*- coding: utf-8 -*-
"""Deep_Learning(part-1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lAffJegzFDuWe_YSwB5ocUjytC72GdE2
"""

import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split

df = pd.DataFrame({
    "soil_moisture": [0.10, 0.15, 0.20, 0.25, 0.40, 0.60, 0.35, 0.18,
                      0.45, 0.05, 0.80, 0.27, 0.55, 0.70, 0.12, 0.30],
    "temperature_c": [34, 30, 26, 22, 28, 30, 19, 22,
                      35, 24, 33, 33, 21, 25, 20, 29],
    "sunlight_hours": [9, 8, 7, 4, 8, 10, 3, 10,
                       12, 5, 9, 11, 2, 6, 1, 9],
    "needs_water": [1, 1, 1, 0, 0, 0, 0, 1,
                    0, 1, 0, 1, 0, 0, 1, 1]
})

df

#now we will split our data into dependent and independent feature

df.columns

x= df.drop('needs_water',axis=1)
y= df['needs_water']

#now as we know in ML we do standardisation which bring every numeric value from -3 to 3
#but in deep learning we use normalization

#now we will do feature scaling using normalization

x_min= x.min()
x_max= x.max()
x_scaled= (x-x_min)/(x_max-x_min+1e-8)

x_scaled

#train_test_split...

x_train,x_test,y_train,y_test= train_test_split(x_scaled,y,test_size=0.25,random_state=42,stratify=y)

#building basic deep learning model....
model= keras.Sequential([
    layers.Input(shape=(x_scaled.shape[1],)), #building input layer
    layers.Dense(8,activation='relu'),        #building hidden layer
    layers.Dense(1,activation='sigmoid')      #building output layer

])

model.compile(optimizer='sgd',loss='binary_crossentropy',
              metrics=['accuracy'])

history= model.fit(x_train.values,y_train.values,
                   validation_data=(x_test.values,y_test.values),
                   epochs=100,batch_size=4,verbose=1)

