# -*- coding: utf-8 -*-
"""Deep_Learning(ANN).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ht1XIhocrcnZ8NyOuzGxdFwpGpBT1Jei
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

#Machine learning utilities...
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder,StandardScaler
from sklearn.linear_model import Perceptron
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report


#deep learning utilities....
import tensorflow as tf
from tensorflow.keras.models import Sequential #helps to build neural network layer by layer
from tensorflow.keras.layers import Dense #fully connected layer
from tensorflow.keras.layers import Dropout #randomly remove some neurons to prevent from overfitting...
from tensorflow.keras.utils import to_categorical

df= pd.read_csv('iris.csv')

df.head()

df['species'].value_counts()

df.info()

sns.pairplot(df,hue='species')

df.columns

#splitting the whole data in to depedent and independent feature

x=df.drop(columns=['species'],axis=1)
y=df['species']

#encoding text into integer...
encoder= LabelEncoder()
encoded_y= encoder.fit_transform(y)

x_train, x_test, y_train,y_test= train_test_split(x,encoded_y,test_size=0.2,random_state=42,stratify=encoded_y)

scaler= StandardScaler()
x_scaled_train= scaler.fit_transform(x_train)
x_scaled_test= scaler.transform(x_test)

#now we are using simplest neural network perceptron

per= Perceptron(max_iter=1000,random_state=42)
per.fit(x_scaled_train,y_train)

p_pred_percep= per.predict(x_scaled_test)

accuracy_score(y_test,p_pred_percep)

print(classification_report(y_test,p_pred_percep))

#now we will use advance neural network Artificial Neural Network....

#we do normalization instead of doing standardization...

#while going through deep learning process we have to do one hot coding instead of label encoding...
#one hot coding is done by using to_categorical function...

y_train_cat= to_categorical(y_train,num_classes=3)
y_test_cat= to_categorical(y_test,num_classes=3)

model= Sequential([
    Dense(16,input_dim=4,activation='relu'),
    Dense(8,activation='relu'),
    Dense(3,activation='softmax')
])

model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])

history= model.fit(x_scaled_train,y_train_cat,epochs=90,batch_size=30,validation_split=0.2,verbose=1)

#hence we successfully achieved 92 percent accuracy, by adjusting batch and echops value....

#sometimes our models get start overfitting then we have to adjust our epoch value and batch size, as our data is very small that's why our model starts overfitting...

#now using history varibale we can plot our accuracy graph..
plt.figure(figsize=(10,4))
plt.plot(history.history['accuracy'], label='train')
plt.plot(history.history['val_accuracy'], label='test')
plt.title("Model Accuracy over Epochs")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()
plt.show()

#by observing this graph we can see that in starting we are having low bias and high variance which means underfitting but in the end we came with our perfect and generalised model..